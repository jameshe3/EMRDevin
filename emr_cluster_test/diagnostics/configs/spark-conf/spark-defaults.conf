spark.databricks.delta.addColumnBeforePartitionColumns.enabled true
spark.executor.logs.rolling.enableCompression false
spark.sql.cacheWithCTE.enabled true
spark.sql.windowTopKFilter.pushdown true
spark.rdd.compress false
spark.scheduler.listenerbus.eventqueue.capacity 10000
spark.sql.adaptive.coalescePartitions.initialPartitionNum 200
spark.sql.shuffle.partitions 200
spark.history.fs.cleaner.enabled false
spark.memory.offHeap.enabled false
spark.storage.memoryMapThreshold 2m
spark.history.fs.cleaner.maxAge 7d
spark.shuffle.compress true
spark.shuffle.io.numConnectionsPerPeer 1
spark.kryo.registrationRequired false
spark.sql.cbo.joinReorder.enabled true
spark.dynamicAllocation.minExecutors 0
spark.yarn.historyServer.address master-1-1.c-dca6391bee134114.cn-hangzhou.emr.aliyuncs.com:18080
spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,com.aliyun.jindodata
spark.task.maxFailures 4
spark.locality.wait.rack 0s
spark.logConf false
spark.shuffle.io.maxRetries 3
spark.kryoserializer.buffer.max 64m
spark.kryo.unsafe false
spark.executor.heartbeatInterval 10s
spark.broadcast.compress true
spark.io.encryption.keygen.algorithm HmacSHA1
spark.broadcast.blockSize 4m
spark.memory.storageFraction 0.5
spark.files.openCostInBytes 4194304
spark.yarn.am.extraJavaOptions -noverify -javaagent:/opt/apps/TAIHAODOCTOR/taihaodoctor-current/emr-agent/btrace-agent.jar=libs=spark-3.2
spark.sql.optimizer.dynamicPartitionPruning.enabled true
spark.sql.distinctAggPushdown.enabled true
spark.executor.logs.rolling.time.interval daily
spark.speculation.multiplier 1.5
spark.dynamicAllocation.enabled false
spark.sql.queryExecutionListeners com.aliyun.emr.meta.spark.listener.EMRQueryLogger
spark.driver.maxResultSize 1g
spark.files.overwrite false
spark.memory.offHeap.size 0
spark.yarn.dist.files /etc/taihao-apps/spark-conf/hive-site.xml
spark.databricks.delta.stats.collect true
spark.eventLog.logBlockUpdates.enabled false
spark.locality.wait.node 0s
spark.driver.extraClassPath /opt/apps/METASTORE/metastore-current/hive2/*:/opt/apps/JINDOSDK/jindosdk-current/lib/*:/opt/apps/EMRHOOK/emrhook-current/spark-hook-1.1.4-spark30.jar
spark.driver.userClassPathFirst false
spark.reducer.maxSizeInFlight 48m
spark.kryo.referenceTracking true
spark.sql.sources.parallelPartitionDiscovery.parallelism 1000
spark.files.maxPartitionBytes 134217728
spark.locality.wait 0s
spark.sql.adaptive.coalescePartitions.minPartitionSize 1MB
spark.executor.instances 2
spark.dynamicAllocation.executorIdleTimeout 60s
spark.sql.cbo.joinReorder.dp.star.filter false
spark.sql.adaptive.skewJoin.enabled false
spark.executor.memory 2948m
spark.shuffle.sort.bypassMergeThreshold 200
spark.dynamicAllocation.schedulerBacklogTimeout 1s
spark.scheduler.minRegisteredResourcesRatio 0.8
spark.cleaner.referenceTracking true
spark.sql.optimizer.dynamicPartitionPruning.useStats true
spark.io.compression.lz4.blockSize 32k
spark.eventLog.compress false
spark.shuffle.registration.timeout 5000
spark.sql.autoBroadcastJoinThreshold 10485760
spark.scheduler.mode FIFO
spark.memory.fraction 0.6
spark.files.useFetchCache true
spark.sql.parquet.output.committer.class com.aliyun.jindodata.commit.JindoCommitter
spark.executor.extraClassPath /opt/apps/METASTORE/metastore-current/hive2/*:/opt/apps/JINDOSDK/jindosdk-current/lib/*:/opt/apps/EMRHOOK/emrhook-current/spark-hook-1.1.4-spark30.jar
spark.io.encryption.keySizeBits 128
spark.executor.extraLibraryPath $LD_LIBRARY_PATH:/opt/apps/YARN/yarn-current/lib/native/:/opt/apps/SPARK3/spark-current/lib/native/
spark.history.fs.cleaner.interval 1d
spark.python.worker.reuse true
spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly true
spark.io.compression.zstd.bufferSize 32k
spark.sql.adaptive.enabled true
spark.locality.wait.process 0s
spark.shuffle.service.index.cache.size 100m
spark.history.fs.logDirectory hdfs://master-1-1.c-dca6391bee134114.cn-hangzhou.emr.aliyuncs.com:9000/spark-history
spark.driver.memory 2g
spark.io.compression.snappy.blockSize 32k
spark.master yarn
spark.scheduler.maxRegisteredResourcesWaitingTime 30s
spark.sql.adaptive.advisoryPartitionSizeInBytes 64MB
spark.driver.cores 1
spark.driver.extraLibraryPath $LD_LIBRARY_PATH:/opt/apps/YARN/yarn-current/lib/native/:/opt/apps/SPARK3/spark-current/lib/native/
spark.files.fetchTimeout 60s
spark.io.compression.codec lz4
spark.shuffle.registration.maxAttempts 3
spark.speculation false
spark.ui.view.acls *
spark.sql.cbo.joinReorder.dp.threshold 12
spark.history.ui.port 18080
spark.sql.cbo.enabled true
spark.shuffle.io.retryWait 5s
spark.eventLog.buffer.kb 100k
spark.driver.extraJavaOptions -Dlog4j2.formatMsgNoLookups=true -Dlog4j.configuration=/etc/taihao-apps/spark-conf/log4j.properties -noverify -javaagent:/opt/apps/TAIHAODOCTOR/taihaodoctor-current/emr-agent/btrace-agent.jar=libs=spark-3.2
spark.eventLog.overwrite false
spark.hadoop.yarn.timeline-service.enabled false
spark.databricks.delta.stats.skipping true
spark.history.fs.numReplayThreads 4
spark.shuffle.spill.compress true
spark.python.worker.memory 512m
spark.storage.replication.proactive false
spark.sql.sources.outputCommitterClass com.aliyun.jindodata.commit.JindoCommitter
spark.sql.adaptive.coalescePartitions.enabled true
spark.eventLog.dir hdfs://master-1-1.c-dca6391bee134114.cn-hangzhou.emr.aliyuncs.com:9000/spark-history
spark.io.compression.zstd.level 1
spark.kryoserializer.buffer 64k
spark.sql.cacheWithCTE.persist.threshold 0MB
spark.databricks.delta.snapshotPartitions 10
spark.shuffle.accurateBlockThreshold 104857600
spark.shuffle.io.preferDirectBufs true
spark.speculation.quantile 0.75
spark.io.encryption.enabled false
spark.executor.cores 1
spark.dynamicAllocation.maxExecutors 200
spark.cleaner.periodicGC.interval 30min
spark.pyspark.python /usr/local/bin/python3.7
spark.task.cpus 1
spark.shuffle.service.port 7337
spark.speculation.interval 100ms
spark.driver.memoryOverhead 1g
spark.executor.userClassPathFirst false
spark.eventLog.enabled true
spark.python.profile false
spark.shuffle.file.buffer 32k
spark.shuffle.service.enabled true
spark.executor.extraJavaOptions -Dlog4j2.formatMsgNoLookups=true -Dlog4j.configuration=/etc/taihao-apps/spark-conf/log4j.properties -noverify -javaagent:/opt/apps/TAIHAODOCTOR/taihaodoctor-current/emr-agent/btrace-agent.jar=libs=spark-3.2
spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio 0.5
spark.driver.supervise false
